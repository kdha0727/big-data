{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Information: Yonsei University CSI4121 2021-1 \\\n",
    "Student Information: Dongha Kim \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report is about Sentiment analysis using RNN and LSTM model. \\\n",
    "In this report, there are code explanation of each implement, \\\n",
    "and evaluation of model.\n",
    "\n",
    "###### this report's code requires python version upper than 3.7!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implements"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialize project - import frameworks and load dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "#\n",
    "# Configurations\n",
    "#\n",
    "\n",
    "# Model Configuration\n",
    "BATCH_SIZE = 64\n",
    "USE_CUDA = True\n",
    "\n",
    "# File I/O Configuration\n",
    "BASE_DIR = './'\n",
    "DATASET_FOLDER = 'data'\n",
    "SNAPSHOT_FOLDER = 'snapshot'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#\n",
    "# Download MeCab tokenizer and load\n",
    "#\n",
    "\n",
    "import os\n",
    "if os.name == \"nt\":  # Windows\n",
    "    !pip install \"eunjeon\"\n",
    "    from eunjeon import Mecab\n",
    "else:\n",
    "    try:\n",
    "        import google.colab\n",
    "    except ImportError:  # Linux, MacOS\n",
    "        pass\n",
    "    else:  # Google Co-lab\n",
    "        !git clone \"https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\"\n",
    "        %cd \"Mecab-ko-for-Google-Colab\"\n",
    "        !bash install_mecab-ko_on_colab190912.sh\n",
    "        %cd ..\n",
    "        !rm \"Mecab-ko-for-Google-Colab\" -rf\n",
    "    finally:\n",
    "        from konlpy.tag import Mecab\n",
    "\n",
    "# --- torchtext patch\n",
    "\n",
    "import torchtext\n",
    "try:\n",
    "    getattr(torchtext, 'legacy')\n",
    "except AttributeError:\n",
    "    import types\n",
    "    import sys\n",
    "    class LegacyModule(types.ModuleType):\n",
    "        data = torchtext.data\n",
    "    torchtext.legacy = LegacyModule\n",
    "    sys.modules['torchtext'] = torchtext\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS version: \t\tmacOS-11.4-arm64-arm-64bit\n",
      "Python version:\t\t3.8.10 | packaged by conda-forge\n",
      "Torch version:\t\t1.8.0\n",
      "Torch device:\t\tcpu\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Import modules and set environments\n",
    "#\n",
    "\n",
    "# Framework import\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # noqa\n",
    "import torch.optim\n",
    "import torchtext\n",
    "if os.name == \"nt\":\n",
    "    from eunjeon import Mecab\n",
    "else:\n",
    "    from konlpy.tag import Mecab\n",
    "\n",
    "# Utility import\n",
    "import sys\n",
    "import os\n",
    "import platform\n",
    "import urllib.request\n",
    "\n",
    "# Reporthook import\n",
    "try:\n",
    "    import reporthook\n",
    "except ImportError:\n",
    "    reporthook = None\n",
    "\n",
    "# Prepare cuda-related variables\n",
    "USE_CUDA = USE_CUDA and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "\n",
    "# Environment check\n",
    "print(\n",
    "    \"OS version: \\t\\t{0}\\nPython version:\\t\\t{1}\\nTorch version:\\t\\t{2}\\nTorch device:\\t\\t{3}\"\n",
    "        .format(platform.platform(), sys.version, torch.__version__, device)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]: 120000 [val]: 30000 [test]: 50000 [words]: 10070 [class] 2\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Prepare dataset\n",
    "#\n",
    "\n",
    "# Set directories\n",
    "BASE_DIR = os.path.abspath(BASE_DIR)\n",
    "dataset_dir = os.path.join(BASE_DIR, DATASET_FOLDER)\n",
    "snapshot_dir = os.path.join(BASE_DIR, SNAPSHOT_FOLDER)\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = Mecab()\n",
    "\n",
    "# Set fields\n",
    "ID = torchtext.legacy.data.Field(\n",
    "    sequential = False,\n",
    "    use_vocab = False)\n",
    "TEXT = torchtext.legacy.data.Field(\n",
    "    sequential=True,\n",
    "    use_vocab=True,\n",
    "    tokenize=tokenizer.morphs,\n",
    "    lower=True,\n",
    "    batch_first=True,\n",
    "    fix_length=20)\n",
    "LABEL = torchtext.legacy.data.Field(\n",
    "    sequential=False,\n",
    "    use_vocab=False,\n",
    "    is_target=True)\n",
    "\n",
    "# Download dataset\n",
    "if not os.path.exists(os.path.join(dataset_dir, \"ratings_train.txt\")):\n",
    "    urllib.request.urlretrieve(\n",
    "        \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\",\n",
    "        filename=os.path.join(dataset_dir, \"ratings_train.txt\"))\n",
    "if not os.path.exists(os.path.join(dataset_dir, \"ratings_test.txt\")):\n",
    "    urllib.request.urlretrieve(\n",
    "        \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\",\n",
    "        filename=os.path.join(dataset_dir, \"ratings_test.txt\"))\n",
    "\n",
    "# Load dataset object and tokenize dataset\n",
    "train_data, test_data = torchtext.legacy.data.TabularDataset.splits(\n",
    "    path=dataset_dir, train='ratings_train.txt', test='ratings_test.txt', format='tsv',\n",
    "    fields=[('id', ID), ('text', TEXT), ('label', LABEL)], skip_header=True)\n",
    "\n",
    "# Make Vocab\n",
    "TEXT.build_vocab(train_data, min_freq=10)  # todo\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# Define number of words and number of labels in the 'word vocabulary'\n",
    "vocab_size = len(TEXT.vocab)\n",
    "n_classes = 2\n",
    "\n",
    "# Split validation data by 8:2\n",
    "train_data, val_data = train_data.split(split_ratio=0.8)\n",
    "\n",
    "# Make iterator-getter\n",
    "def get_iterator(dataset):\n",
    "    return torchtext.legacy.data.Iterator(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "# Summarize dataset\n",
    "print(\n",
    "    \"[train]: %d [val]: %d [test]: %d [words]: %d [class] %d\" %\n",
    "    (len(train_data),len(val_data), len(test_data), vocab_size, n_classes)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement of RNN Model and LSTM model\n",
    "First, implement the RNN and LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# RNN Model implement\n",
    "#\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \"\"\"RNN model implement for Sentiment-analysis\"\"\"\n",
    "\n",
    "    # Constructor\n",
    "    # Register layers in constructor\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_vocab,\n",
    "            num_classes,\n",
    "            num_layers=1,\n",
    "            embed_size=128,\n",
    "            hidden_size=256,\n",
    "            dropout_p=0.2\n",
    "    ):\n",
    "        nn.Module.__init__(self)\n",
    "        self.num_layers = num_layers # number of layer\n",
    "        self.hidden_size = hidden_size # hidden layer dimension\n",
    "        self.embed = nn.Embedding(num_vocab, embed_size) # n_vocab = number of words in the vocab. / embed_size = dimension of embedding\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, num_classes) # For classification\n",
    "\n",
    "    # Implement forward\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        h_0 = self._init_state(batch_size=x.size(0)) # Initialize hidden state\n",
    "        x, _ = self.rnn(x, h_0)  # [batch_size, sequence length, hidden_dim]\n",
    "        h_t = x[:,-1,:] # [batch_size, hidden_dim]\n",
    "        self.dropout(h_t)\n",
    "        logit = self.out(h_t)  # [batch_size, hidden_dim] -> [batch_size, n_classes]\n",
    "        return logit\n",
    "\n",
    "    def _init_state(self, batch_size=1):\n",
    "        weight = next(self.parameters()).data\n",
    "        return weight.new(self.num_layers, batch_size, self.hidden_size).zero_()\n",
    "\n",
    "    # Backward will be implemented automatically,\n",
    "    # by the pytorch framework itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#\n",
    "# LSTM Model implement\n",
    "#\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"LSTM model implement for Sentiment-analysis\"\"\"\n",
    "\n",
    "    # Constructor\n",
    "    # Register layers in constructor\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_vocab,\n",
    "            num_classes,\n",
    "            num_layers=1,\n",
    "            embed_size=128,\n",
    "            hidden_size=256,\n",
    "            dropout_p=0.2\n",
    "    ):\n",
    "        nn.Module.__init__(self)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(num_vocab, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.embed(x)\n",
    "\n",
    "        # Initialize the hidden_state and cell_state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0),\n",
    "                         self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0),\n",
    "                         self.hidden_size).to(device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0)) # [batch_size, sequence length, hidden_dim]\n",
    "        ht = out[:,-1,:] # [batch_size, hidden_dim]\n",
    "        self.dropout(ht)\n",
    "\n",
    "        logit = self.out(ht) # [batch_size, hidden_dim] -> [batch_size, n_classes]\n",
    "\n",
    "        return logit\n",
    "\n",
    "    # Backward will be implemented automatically,\n",
    "    # by the pytorch framework itself.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement of train / test functions\n",
    "Next to the model, I implemented train and test functions. \\\n",
    "Since pytorch has autograd feature, training and testing process can occur \\\n",
    "with some simple operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Train / Test / Evaluate function implements\n",
    "#\n",
    "\n",
    "\n",
    "def train_model(\n",
    "        model,\n",
    "        train_iter,\n",
    "        optimizer,\n",
    "        log_hook=None,\n",
    "        log_interval=50\n",
    "):\n",
    "    \"\"\"\n",
    "    Training function with using model, data loader, optimizer.\n",
    "\n",
    "    :param model: torch.nn.Module\n",
    "    :param train_iter:\n",
    "    :param optimizer: torch.optim.Optimizer\n",
    "    :param log_hook: (optional) when logging into stdout or gui, you can supply\n",
    "                    logging hook callable, or class that has 'train' attribute\n",
    "    :param log_interval: (optional) supply when using log hook. default is 10\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # Set train mode\n",
    "    model.train()\n",
    "    # Iteration\n",
    "    for iteration, batch in enumerate(train_iter):\n",
    "        x, y = batch.text.to(device), batch.label.to(device)\n",
    "        # Zero grad\n",
    "        optimizer.zero_grad()\n",
    "        # Calculate with model and evaluate (forward propagation)\n",
    "        output = model(x)  # Calculate by calling model\n",
    "        loss = F.cross_entropy(output, y)  # Evaluate\n",
    "        # Optimize model by gradient (backward propagation)\n",
    "        loss.backward()  # get gradient with backward propagation\n",
    "        optimizer.step()  # step: optimize weight params with gradient\n",
    "        # Log train information\n",
    "        if iteration % log_interval == 0 and log_hook is not None:\n",
    "            getattr(log_hook, 'train', log_hook)(loss, iteration)\n",
    "\n",
    "\n",
    "@torch.no_grad()  # stop autograd progress\n",
    "def evaluate_model(\n",
    "    model, val_iter,\n",
    "    log_hook=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluating function with using model, data loader.\n",
    "\n",
    "    :param model: torch.nn.Module\n",
    "    :param val_iter:\n",
    "    :param log_hook: (optional) when logging into stdout or gui, you can supply\n",
    "                    logging hook callable, or class that has 'test' attribute\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # Set test mode\n",
    "    model.eval()\n",
    "    # Initialize values\n",
    "    total_loss, corrects = 0., 0\n",
    "    # Iteration\n",
    "    for batch in val_iter:\n",
    "        # Convert device\n",
    "        x, y = batch.text.to(device), batch.label.to(device)\n",
    "        # Calculate with model\n",
    "        output = model(x)\n",
    "        # Append test-loss value\n",
    "        total_loss += F.cross_entropy(output, y, reduction='sum').item()\n",
    "        # Add accuracy count\n",
    "        corrects += (output.max(1)[1].view(y.size()).data == y.data).sum()  # noqa\n",
    "    # Calculate average test loss\n",
    "    size = len(val_iter.dataset)\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = corrects / size\n",
    "    # Convert torch tensor to float\n",
    "    if hasattr(avg_loss, 'item'):\n",
    "        avg_loss = avg_loss.item()\n",
    "    if hasattr(avg_accuracy, 'item'):\n",
    "        avg_accuracy = avg_accuracy.item()\n",
    "    # Log test information\n",
    "    if log_hook is not None:\n",
    "        getattr(log_hook, 'evaluate', log_hook)(avg_loss, avg_accuracy)\n",
    "    return avg_loss, avg_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<Start Learning> total 20 epochs\n",
      "\n",
      "Epoch 1\n",
      "[Train]\t Progress: 118400/120000 (98.67%), \tLoss: 0.641018 \n",
      "[Eval]\t Average loss: 0.65580, \t\tTotal accuracy: 63.11%\n",
      "\n",
      "Epoch 2\n",
      "[Train]\t Progress: 118400/120000 (98.67%), \tLoss: 0.548438 \n",
      "[Eval]\t Average loss: 0.47493, \t\tTotal accuracy: 77.88%\n",
      "\n",
      "Epoch 3\n",
      "[Train]\t Progress: 118400/120000 (98.67%), \tLoss: 0.506033 \n",
      "[Eval]\t Average loss: 0.45873, \t\tTotal accuracy: 79.27%\n",
      "\n",
      "Epoch 4\n",
      "[Train]\t Progress: 118400/120000 (98.67%), \tLoss: 0.387619 \n",
      "[Eval]\t Average loss: 0.45697, \t\tTotal accuracy: 80.46%\n",
      "\n",
      "Epoch 5\n",
      "[Train]\t Progress: 118400/120000 (98.67%), \tLoss: 0.316389 \n",
      "[Eval]\t Average loss: 0.41577, \t\tTotal accuracy: 81.91%\n",
      "\n",
      "Epoch 6\n",
      "[Train]\t Progress: 118400/120000 (98.67%), \tLoss: 0.385587 \n",
      "[Eval]\t Average loss: 0.41560, \t\tTotal accuracy: 82.06%\n",
      "\n",
      "Epoch 7\n",
      "[Train]\t Progress: 118400/120000 (98.67%), \tLoss: 0.334940 \n",
      "[Eval]\t Average loss: 0.41101, \t\tTotal accuracy: 81.61%\n",
      "\n",
      "Epoch 8\n",
      "[Train]\t Progress: 118400/120000 (98.67%), \tLoss: 0.524896 \n",
      "[Eval]\t Average loss: 0.41788, \t\tTotal accuracy: 81.93%\n",
      "\n",
      "Epoch 9\n",
      "[Train]\t Progress: 118400/120000 (98.67%), \tLoss: 0.387898 \n",
      "[Eval]\t Average loss: 0.39787, \t\tTotal accuracy: 82.64%\n",
      "\n",
      "Epoch 10\n",
      "[Train]\t Progress: 118400/120000 (98.67%), \tLoss: 0.278033 \n",
      "[Eval]\t Average loss: 0.40105, \t\tTotal accuracy: 82.78%\n",
      "\n",
      "Epoch 11\n",
      "[Train]\t Progress: 118400/120000 (98.67%), \tLoss: 0.262872 \n",
      "[Eval]\t Average loss: 0.39736, \t\tTotal accuracy: 82.64%\n",
      "\n",
      "Epoch 12\n",
      "[Train]\t Progress: 118400/120000 (98.67%), \tLoss: 0.295667 \n",
      "[Eval]\t Average loss: 0.39793, \t\tTotal accuracy: 82.13%\n",
      "\n",
      "Epoch 13\n",
      "[Train]\t Progress: 118400/120000 (98.67%), \tLoss: 0.263858 \n",
      "[Eval]\t Average loss: 0.42451, \t\tTotal accuracy: 82.61%\n",
      "\n",
      "Epoch 14\n",
      "[Train]\t Progress: 118400/120000 (98.67%), \tLoss: 0.364489 \n",
      "[Eval]\t Average loss: 0.40658, \t\tTotal accuracy: 82.53%\n",
      "\n",
      "Epoch 15\n",
      "[Train]\t Progress: 118400/120000 (98.67%), \tLoss: 0.337719 \n",
      "[Eval]\t Average loss: 0.39610, \t\tTotal accuracy: 82.98%\n",
      "\n",
      "Epoch 16\n",
      "[Train]\t Progress: 118400/120000 (98.67%), \tLoss: 0.316354 \n",
      "[Eval]\t Average loss: 0.40700, \t\tTotal accuracy: 82.69%\n",
      "\n",
      "Epoch 17\n",
      "[Train]\t Progress: 118400/120000 (98.67%), \tLoss: 0.305014 \n",
      "[Eval]\t Average loss: 0.42586, \t\tTotal accuracy: 82.36%\n",
      "\n",
      "Epoch 18\n",
      "[Train]\t Progress: 118400/120000 (98.67%), \tLoss: 0.375999 \n",
      "[Eval]\t Average loss: 0.40312, \t\tTotal accuracy: 82.46%\n",
      "\n",
      "Epoch 19\n",
      "[Train]\t Progress: 118400/120000 (98.67%), \tLoss: 0.272026 \n",
      "[Eval]\t Average loss: 0.46821, \t\tTotal accuracy: 82.56%\n",
      "\n",
      "Epoch 20\n",
      "[Train]\t Progress: 118400/120000 (98.67%), \tLoss: 0.336114 \n",
      "[Eval]\t Average loss: 0.41339, \t\tTotal accuracy: 82.93%\n",
      "\n",
      "\n",
      "<Stop Learning> Least loss: 0.39610468180974323\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "RNN(\n  (embed): Embedding(10070, 128)\n  (dropout): Dropout(p=0.2, inplace=False)\n  (rnn): RNN(128, 256, batch_first=True)\n  (out): Linear(in_features=256, out_features=2, bias=True)\n)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Actual running model-learn function implement\n",
    "#\n",
    "\n",
    "def run_model_learning(\n",
    "        epoch=20,\n",
    "        model_class=RNN,\n",
    "        model_params=None,\n",
    "        optimizer_class=torch.optim.Adam,\n",
    "        learning_rate=1e-3,\n",
    "        verbose=False,\n",
    "        filename=None,\n",
    "        **_model_params\n",
    "):\n",
    "    \"\"\"\n",
    "    ACTUAL function that executes model learning, by given epoch and train data ratio.\n",
    "\n",
    "    :param epoch: (int) epochs\n",
    "    :param model_class: model class\n",
    "    :param model_params: (dict) model parameters\n",
    "    :param optimizer_class: optimizer class\n",
    "    :param learning_rate: (float) learning rate\n",
    "    :param verbose: (bool) verbosity. with turning it on, you can view learning logs.\n",
    "    :param filename: (str) In this path name, model's weight parameter will be saved.\n",
    "\n",
    "    :return: trained-model.\n",
    "    \"\"\"\n",
    "\n",
    "    verbose = verbose and reporthook is not None\n",
    "\n",
    "    # Prepare datasets\n",
    "    train_iter, val_iter = get_iterator(train_data), get_iterator(val_data)\n",
    "\n",
    "    # Initialize model\n",
    "    kwargs = dict(\n",
    "        num_vocab=vocab_size,\n",
    "        num_classes=n_classes,\n",
    "    )\n",
    "    kwargs.update(model_params or {})\n",
    "    kwargs.update(_model_params)\n",
    "    model = model_class(**kwargs).to(device)\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = optimizer_class(\n",
    "        model.parameters(), lr=learning_rate\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n<Start Learning> total {epoch} epochs\", end='\\n\\n')\n",
    "\n",
    "    os.makedirs(snapshot_dir, exist_ok=True)\n",
    "    processing_fn = os.path.join(snapshot_dir, '_processing.pt')\n",
    "    best_val_loss = None\n",
    "\n",
    "    # Do each epoch\n",
    "    for index in range(1, epoch + 1):\n",
    "\n",
    "        rph = reporthook.LearningReporthook(index, train_iter, val_iter) if verbose else None\n",
    "\n",
    "        # Train model.\n",
    "        train_model(\n",
    "            model, train_iter, optimizer, log_hook=rph\n",
    "        )\n",
    "\n",
    "        l, a = evaluate_model(\n",
    "            model, val_iter, log_hook=rph\n",
    "        )\n",
    "\n",
    "        # Save the model having the smallest validation loss\n",
    "        if not best_val_loss or l < best_val_loss:\n",
    "            torch.save(model.state_dict(), processing_fn)\n",
    "            best_val_loss = l\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n<Stop Learning> Least loss: {best_val_loss}\", end='\\n\\n')\n",
    "\n",
    "    model.load_state_dict(torch.load(processing_fn))\n",
    "    os.remove(processing_fn)\n",
    "\n",
    "    if filename is not None:\n",
    "        torch.save(model.state_dict(), os.path.join(snapshot_dir, filename))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Example output\n",
    "run_model_learning(verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluation of Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
